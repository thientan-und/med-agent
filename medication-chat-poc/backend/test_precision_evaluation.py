#!/usr/bin/env python3
"""
Comprehensive Precision Medical AI Evaluation
=============================================
Tests the precision-oriented agentic system with Qdrant RAG data
Evaluates evidence-first routing, precision critic, uncertainty quantification
"""

import asyncio
import json
import sys
import os
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Add backend to path
sys.path.append(os.path.join(os.path.dirname(__file__), '.'))

from app.core.precision_service import PrecisionMedicalAI
from app.core.types import DiagnosisCard, RouteSignals, CriticResult
from app.services.ollama_client import ollama_client

@dataclass
class EvaluationCase:
    """Test case for precision evaluation"""
    id: str
    category: str
    symptoms: str
    expected_route_signals: Dict[str, bool]
    expected_abstention: bool
    expected_safety_certainty: float
    expected_emergency: bool
    description: str
    patient_context: Optional[Dict] = None

@dataclass
class EvaluationResult:
    """Result of precision evaluation"""
    case_id: str
    success: bool
    diagnosis_card: Optional[DiagnosisCard]
    critic_result: Optional[CriticResult]
    route_signals: Optional[RouteSignals]
    abstention_correct: bool
    safety_threshold_met: bool
    emergency_detection_correct: bool
    processing_time_ms: float
    error: Optional[str] = None

class PrecisionEvaluator:
    """Evaluates the precision medical AI system"""

    def __init__(self):
        self.precision_service = PrecisionMedicalAI()
        self.test_cases = self._create_test_cases()
        self.results: List[EvaluationResult] = []

    def _create_test_cases(self) -> List[EvaluationCase]:
        """Create comprehensive test cases for precision evaluation"""
        return [
            # Basic symptoms - should trigger conservative abstention
            EvaluationCase(
                id="basic_001",
                category="Basic Symptoms",
                symptoms="à¸›à¸§à¸”à¸«à¸±à¸§ à¹€à¸›à¹‡à¸™à¹„à¸‚à¹‰",
                expected_route_signals={"fever": True, "severe_headache": False},
                expected_abstention=True,
                expected_safety_certainty=0.8,
                expected_emergency=False,
                description="Basic fever and headache - should abstain conservatively"
            ),

            # Emergency scenario - should escalate immediately
            EvaluationCase(
                id="emergency_001",
                category="Emergency - Chest Pain",
                symptoms="à¸›à¸§à¸”à¸«à¸™à¹‰à¸²à¸­à¸à¹€à¸‰à¸µà¸¢à¸šà¸à¸¥à¸±à¸™ à¸«à¸²à¸¢à¹ƒà¸ˆà¹„à¸¡à¹ˆà¸­à¸­à¸ à¹€à¸£à¹ˆà¸‡à¸”à¹ˆà¸§à¸™",
                expected_route_signals={"chest_pain": True, "breathing_difficulty": True, "emergency_keywords": ["à¹€à¸£à¹ˆà¸‡à¸”à¹ˆà¸§à¸™"]},
                expected_abstention=True,
                expected_safety_certainty=0.9,
                expected_emergency=True,
                description="Emergency chest pain - should escalate to physician",
                patient_context={"age": 55, "gender": "male", "history": "hypertension"}
            ),

            # Meningitis without red flags - should be downgraded by critic
            EvaluationCase(
                id="critic_001",
                category="Critic Test - Meningitis",
                symptoms="à¸›à¸§à¸”à¸«à¸±à¸§ à¸¡à¸µà¹„à¸‚à¹‰ 38.5 à¸­à¸‡à¸¨à¸²",
                expected_route_signals={"fever": True, "severe_headache": False},
                expected_abstention=True,
                expected_safety_certainty=0.85,
                expected_emergency=False,
                description="Fever + headache but no neck stiffness - critic should prevent meningitis diagnosis"
            ),

            # Clear emergency with red flags
            EvaluationCase(
                id="emergency_002",
                category="Emergency - Meningitis Red Flags",
                symptoms="à¸›à¸§à¸”à¸«à¸±à¸§à¸£à¸¸à¸™à¹à¸£à¸‡ à¸„à¸­à¹à¸‚à¹‡à¸‡ à¹€à¸à¸¥à¸µà¸¢à¸”à¹à¸ªà¸‡ à¸‹à¸¶à¸¡ à¸¡à¸µà¹„à¸‚à¹‰à¸ªà¸¹à¸‡",
                expected_route_signals={"fever": True, "severe_headache": True, "neurological_deficit": True},
                expected_abstention=True,
                expected_safety_certainty=0.9,
                expected_emergency=True,
                description="Classic meningitis signs - should escalate immediately"
            ),

            # Calculator input test - chest pain for HEART Score
            EvaluationCase(
                id="calculator_001",
                category="Calculator - HEART Score",
                symptoms="à¹€à¸ˆà¹‡à¸šà¸«à¸™à¹‰à¸²à¸­à¸ à¸›à¸§à¸”à¹à¸™à¹ˆà¸™ à¸­à¸²à¸¢à¸¸ 45 à¸›à¸µ à¹€à¸„à¸¢à¸¡à¸µà¸›à¸±à¸à¸«à¸²à¸«à¸±à¸§à¹ƒà¸ˆ",
                expected_route_signals={"chest_pain": True},
                expected_abstention=False,
                expected_safety_certainty=0.85,
                expected_emergency=False,
                description="Chest pain with sufficient data for HEART Score",
                patient_context={"age": 45, "history": "cardiac", "chest_pain_character": "pressure"}
            ),

            # Uncertainty test - vague symptoms
            EvaluationCase(
                id="uncertainty_001",
                category="High Uncertainty",
                symptoms="à¹„à¸¡à¹ˆà¸ªà¸šà¸²à¸¢ à¹€à¸«à¸™à¸·à¹ˆà¸­à¸¢",
                expected_route_signals={},
                expected_abstention=True,
                expected_safety_certainty=0.6,
                expected_emergency=False,
                description="Vague symptoms - high uncertainty should trigger abstention"
            ),

            # VOI questioning scenario
            EvaluationCase(
                id="voi_001",
                category="Value of Information",
                symptoms="à¹€à¸ˆà¹‡à¸šà¸—à¹‰à¸­à¸‡",
                expected_route_signals={"abdominal_pain": True},
                expected_abstention=True,
                expected_safety_certainty=0.7,
                expected_emergency=False,
                description="Abdominal pain - should generate VOI questions for more specificity"
            ),

            # Thai dialect normalization test
            EvaluationCase(
                id="dialect_001",
                category="Thai Dialect",
                symptoms="à¸ˆà¸¸à¸à¹à¸¥à¹‰à¸§ à¹à¸«à¸‡à¹‚à¸à¸”à¸«à¸±à¸‡ à¸•à¸±à¸§à¸£à¹‰à¸­à¸™",  # Northern dialect for severe pain + fever
                expected_route_signals={"fever": True},
                expected_abstention=True,
                expected_safety_certainty=0.8,
                expected_emergency=False,
                description="Northern Thai dialect - should normalize and route correctly"
            ),

            # Treatment guideline citation test
            EvaluationCase(
                id="treatment_001",
                category="Treatment Guidelines",
                symptoms="à¸•à¸´à¸”à¹€à¸Šà¸·à¹‰à¸­à¸—à¸²à¸‡à¹€à¸”à¸´à¸™à¸›à¸±à¸ªà¸ªà¸²à¸§à¸° à¸›à¸±à¸ªà¸ªà¸²à¸§à¸°à¹à¸ªà¸š à¸›à¸±à¸ªà¸ªà¸²à¸§à¸°à¸šà¹ˆà¸­à¸¢",
                expected_route_signals={},
                expected_abstention=False,
                expected_safety_certainty=0.85,
                expected_emergency=False,
                description="UTI symptoms - treatment recommendations must have guideline citations"
            )
        ]

    async def evaluate_single_case(self, case: EvaluationCase) -> EvaluationResult:
        """Evaluate a single test case"""
        print(f"\n{'='*60}")
        print(f"ğŸ§ª Testing Case: {case.id} - {case.category}")
        print(f"ğŸ“ {case.description}")
        print(f"ğŸ¥ Symptoms: {case.symptoms}")
        print(f"{'='*60}")

        start_time = time.time()

        try:
            # Test the precision service
            diagnosis_card = await self.precision_service.process_medical_consultation(
                message=case.symptoms,
                patient_data=case.patient_context,
                session_id=f"eval_{case.id}"
            )

            processing_time = (time.time() - start_time) * 1000

            # Extract route signals for validation
            route_signals = RouteSignals.from_symptoms(case.symptoms)

            # Run precision critic
            from app.core.critic import create_precision_critic
            critic = create_precision_critic()
            critic_result = critic.validate_diagnosis_card(diagnosis_card)

            # Validate results
            abstention_correct = self._validate_abstention(case, diagnosis_card)
            safety_threshold_met = diagnosis_card.uncertainty.safety_certainty >= 0.85
            emergency_detection_correct = self._validate_emergency_detection(case, diagnosis_card)

            # Print detailed results
            self._print_case_results(case, diagnosis_card, route_signals, critic_result, processing_time)

            return EvaluationResult(
                case_id=case.id,
                success=True,
                diagnosis_card=diagnosis_card,
                critic_result=critic_result,
                route_signals=route_signals,
                abstention_correct=abstention_correct,
                safety_threshold_met=safety_threshold_met,
                emergency_detection_correct=emergency_detection_correct,
                processing_time_ms=processing_time
            )

        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            print(f"âŒ Error: {str(e)}")

            return EvaluationResult(
                case_id=case.id,
                success=False,
                diagnosis_card=None,
                critic_result=None,
                route_signals=None,
                abstention_correct=False,
                safety_threshold_met=False,
                emergency_detection_correct=False,
                processing_time_ms=processing_time,
                error=str(e)
            )

    def _validate_abstention(self, case: EvaluationCase, diagnosis_card: DiagnosisCard) -> bool:
        """Validate if abstention decision matches expectation"""
        has_abstention = diagnosis_card.uncertainty.abstention_reason is not None
        return has_abstention == case.expected_abstention

    def _validate_emergency_detection(self, case: EvaluationCase, diagnosis_card: DiagnosisCard) -> bool:
        """Validate emergency detection"""
        # Check if emergency triage level or escalation is present
        triage_level = diagnosis_card.triage.get("level")
        is_emergency = triage_level in ["emergency", "resuscitation"] if triage_level else False
        return is_emergency == case.expected_emergency

    def _print_case_results(self, case: EvaluationCase, diagnosis_card: DiagnosisCard,
                           route_signals: RouteSignals, critic_result: CriticResult,
                           processing_time: float):
        """Print detailed results for a test case"""

        print(f"\nğŸ“Š RESULTS:")
        print(f"â±ï¸  Processing Time: {processing_time:.0f}ms")

        # Route signals
        print(f"\nğŸ§­ Route Signals:")
        signals_dict = route_signals.dict()
        for key, value in signals_dict.items():
            if value and key != "emergency_keywords":
                print(f"   âœ… {key}: {value}")
            elif key == "emergency_keywords" and value:
                print(f"   ğŸš¨ emergency_keywords: {value}")

        # Differential diagnosis
        print(f"\nğŸ©º Differential Diagnosis:")
        for i, dx in enumerate(diagnosis_card.differential[:3]):
            print(f"   {i+1}. {dx.label} ({dx.icd10}) - {dx.p:.1%}")
            if dx.evidence.for_:
                print(f"      Evidence: {', '.join(dx.evidence.for_[:2])}")
            if dx.evidence.citations:
                print(f"      Citations: {', '.join(dx.evidence.citations[:2])}")

        # Uncertainty metrics
        print(f"\nğŸ“ˆ Uncertainty Metrics:")
        print(f"   Safety Certainty: {diagnosis_card.uncertainty.safety_certainty:.2f}")
        print(f"   Diagnostic Coverage: {diagnosis_card.uncertainty.diagnostic_coverage:.2f}")
        print(f"   Prediction Set Size: {diagnosis_card.uncertainty.prediction_set_size}")

        if diagnosis_card.uncertainty.abstention_reason:
            print(f"   ğŸš« Abstention: {diagnosis_card.uncertainty.abstention_reason}")

        # Critic results
        print(f"\nğŸ” Critic Validation:")
        print(f"   Passed: {'âœ…' if critic_result.passed else 'âŒ'}")
        if critic_result.failed_rules:
            print(f"   Failed Rules: {', '.join(critic_result.failed_rules)}")
        if critic_result.actions:
            print(f"   Required Actions: {', '.join(critic_result.actions)}")

        # Medical calculators
        if diagnosis_card.calculators:
            print(f"\nğŸ§® Medical Calculators:")
            for calc in diagnosis_card.calculators:
                print(f"   {calc.name}: {calc.score} ({calc.risk_band})")
                print(f"     Confidence: {calc.confidence:.2f}")

        # Treatment recommendations
        if diagnosis_card.treatment_candidates:
            print(f"\nğŸ’Š Treatment Recommendations:")
            for treatment in diagnosis_card.treatment_candidates:
                print(f"   {treatment.instructions}")
                if treatment.evidence.citations:
                    print(f"     Citations: {', '.join(treatment.evidence.citations)}")
                print(f"     Safety Score: {treatment.safety_score:.2f}")

        # Overall assessment
        print(f"\nğŸ¯ Validation Results:")
        abstention_expected = case.expected_abstention
        abstention_actual = diagnosis_card.uncertainty.abstention_reason is not None
        print(f"   Abstention: {'âœ…' if abstention_actual == abstention_expected else 'âŒ'} "
              f"(Expected: {abstention_expected}, Actual: {abstention_actual})")

        safety_met = diagnosis_card.uncertainty.safety_certainty >= 0.85
        print(f"   Safety Threshold: {'âœ…' if safety_met else 'âŒ'} "
              f"({diagnosis_card.uncertainty.safety_certainty:.2f} >= 0.85)")

    async def run_comprehensive_evaluation(self) -> Dict[str, Any]:
        """Run the complete precision evaluation"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ PRECISION MEDICAL AI COMPREHENSIVE EVALUATION")
        print(f"{'='*80}")
        print(f"ğŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ§ª Test Cases: {len(self.test_cases)}")

        # Initialize services
        print(f"\nğŸš€ Precision service ready...")

        # Check Ollama connection
        print(f"ğŸ¤– Checking Ollama connection...")
        ollama_connected = await ollama_client.check_connection()
        if ollama_connected:
            models = await ollama_client.list_models()
            print(f"âœ… Ollama connected. Models: {models}")
        else:
            print(f"âš ï¸ Ollama not connected - using fallback mode")

        # Run evaluation cases
        for case in self.test_cases:
            result = await self.evaluate_single_case(case)
            self.results.append(result)

            # Brief pause between tests
            await asyncio.sleep(1)

        # Generate summary
        return self._generate_evaluation_summary()

    def _generate_evaluation_summary(self) -> Dict[str, Any]:
        """Generate comprehensive evaluation summary"""
        total_cases = len(self.results)
        successful_cases = len([r for r in self.results if r.success])
        failed_cases = total_cases - successful_cases

        # Precision metrics
        abstention_correct = len([r for r in self.results if r.abstention_correct])
        safety_threshold_met = len([r for r in self.results if r.safety_threshold_met])
        emergency_detection_correct = len([r for r in self.results if r.emergency_detection_correct])

        # Critic metrics
        critic_passed = len([r for r in self.results if r.critic_result and r.critic_result.passed])

        # Performance metrics
        avg_processing_time = sum(r.processing_time_ms for r in self.results) / total_cases

        # Calculate precision scores
        abstention_accuracy = abstention_correct / total_cases
        safety_compliance = safety_threshold_met / total_cases
        emergency_accuracy = emergency_detection_correct / total_cases
        critic_pass_rate = critic_passed / total_cases

        summary = {
            "evaluation_completed": datetime.now().isoformat(),
            "total_test_cases": total_cases,
            "successful_cases": successful_cases,
            "failed_cases": failed_cases,
            "success_rate": successful_cases / total_cases,

            # Precision metrics
            "precision_metrics": {
                "abstention_accuracy": abstention_accuracy,
                "safety_compliance_rate": safety_compliance,
                "emergency_detection_accuracy": emergency_accuracy,
                "critic_pass_rate": critic_pass_rate
            },

            # Performance metrics
            "performance_metrics": {
                "average_processing_time_ms": avg_processing_time,
                "total_evaluation_time_ms": sum(r.processing_time_ms for r in self.results)
            },

            # Detailed results
            "case_results": [
                {
                    "case_id": r.case_id,
                    "success": r.success,
                    "abstention_correct": r.abstention_correct,
                    "safety_threshold_met": r.safety_threshold_met,
                    "emergency_detection_correct": r.emergency_detection_correct,
                    "processing_time_ms": r.processing_time_ms,
                    "error": r.error
                }
                for r in self.results
            ]
        }

        return summary

    def print_final_summary(self, summary: Dict[str, Any]):
        """Print the final evaluation summary"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š PRECISION EVALUATION SUMMARY")
        print(f"{'='*80}")

        # Overall results
        print(f"\nğŸ¯ Overall Results:")
        print(f"   Total Test Cases: {summary['total_test_cases']}")
        print(f"   Successful: {summary['successful_cases']} ({summary['success_rate']:.1%})")
        print(f"   Failed: {summary['failed_cases']}")

        # Precision metrics
        metrics = summary['precision_metrics']
        print(f"\nğŸ“ˆ Precision Metrics:")
        print(f"   Abstention Accuracy: {metrics['abstention_accuracy']:.1%}")
        print(f"   Safety Compliance: {metrics['safety_compliance_rate']:.1%}")
        print(f"   Emergency Detection: {metrics['emergency_detection_accuracy']:.1%}")
        print(f"   Critic Pass Rate: {metrics['critic_pass_rate']:.1%}")

        # Performance metrics
        perf = summary['performance_metrics']
        print(f"\nâš¡ Performance Metrics:")
        print(f"   Average Processing Time: {perf['average_processing_time_ms']:.0f}ms")
        print(f"   Total Evaluation Time: {perf['total_evaluation_time_ms']:.0f}ms")

        # Quality assessment
        print(f"\nğŸ† Quality Assessment:")
        overall_precision = (
            metrics['abstention_accuracy'] +
            metrics['safety_compliance_rate'] +
            metrics['emergency_detection_accuracy'] +
            metrics['critic_pass_rate']
        ) / 4

        if overall_precision >= 0.9:
            print(f"   âœ… EXCELLENT: {overall_precision:.1%} precision achieved")
        elif overall_precision >= 0.8:
            print(f"   ğŸ‘ GOOD: {overall_precision:.1%} precision achieved")
        elif overall_precision >= 0.7:
            print(f"   âš ï¸ ACCEPTABLE: {overall_precision:.1%} precision achieved")
        else:
            print(f"   âŒ NEEDS IMPROVEMENT: {overall_precision:.1%} precision achieved")

        # Case-by-case results
        print(f"\nğŸ“‹ Case Results:")
        for case_result in summary['case_results']:
            status = "âœ…" if case_result['success'] else "âŒ"
            print(f"   {status} {case_result['case_id']}: "
                  f"{case_result['processing_time_ms']:.0f}ms")
            if case_result['error']:
                print(f"      Error: {case_result['error']}")

        print(f"\nğŸ‰ Evaluation completed at {summary['evaluation_completed']}")

async def main():
    """Main evaluation function"""
    evaluator = PrecisionEvaluator()

    try:
        # Run comprehensive evaluation
        summary = await evaluator.run_comprehensive_evaluation()

        # Print final summary
        evaluator.print_final_summary(summary)

        # Save results to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"precision_evaluation_results_{timestamp}.json"

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ Results saved to: {results_file}")

    except Exception as e:
        print(f"âŒ EVALUATION FAILED: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # Cleanup
        print(f"ğŸ§¹ Evaluation cleanup completed")

if __name__ == "__main__":
    asyncio.run(main())