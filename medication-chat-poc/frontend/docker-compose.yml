version: '3.8'

services:
  medical-chat-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: medical-chat-app
    restart: unless-stopped

    ports:
      - "3000:3000"  # Main application port
      - "3003:3003"  # WebSocket port

    environment:
      - NODE_ENV=production
      - PORT=3000
      - WEBSOCKET_PORT=3003

      # Ollama Configuration
      # Update this to your production Ollama server
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}

      # Vector Database Configuration
      - QDRANT_URL=${QDRANT_URL}
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - QDRANT_COLLECTION_NAME=${QDRANT_COLLECTION_NAME:-medical_knowledge}

      # Backup Translation Service
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}

      # Security
      - JWT_SECRET=${JWT_SECRET:-change_this_in_production}
      - SESSION_SECRET=${SESSION_SECRET:-change_this_in_production}

      # Feature Flags
      - ENABLE_RAG=${ENABLE_RAG:-true}
      - ENABLE_WEBSOCKET=${ENABLE_WEBSOCKET:-true}
      - ENABLE_ANALYTICS=${ENABLE_ANALYTICS:-false}

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - LOG_FORMAT=${LOG_FORMAT:-json}

      # WebSocket Configuration
      - WEBSOCKET_CORS_ORIGIN=${WEBSOCKET_CORS_ORIGIN:-*}

      # Health Check
      - HEALTH_CHECK_INTERVAL=${HEALTH_CHECK_INTERVAL:-30000}
      - HEALTH_CHECK_TIMEOUT=${HEALTH_CHECK_TIMEOUT:-10000}

    volumes:
      # Mount data directory for medical knowledge
      - ./data:/app/data:ro

      # Mount logs directory (optional)
      - ./logs:/app/logs

    networks:
      - medical-app-network

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    depends_on:
      - qdrant

    # Resource limits
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # Optional: Include Qdrant if you want to run it locally
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant-db
    restart: unless-stopped

    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API

    volumes:
      - qdrant_data:/qdrant/storage

    networks:
      - medical-app-network

    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334

    # Uncomment if you want to use local Qdrant instead of cloud
    # profiles:
    #   - local-db

  # Optional: Include Ollama if you want to run it locally
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: unless-stopped

    ports:
      - "11434:11434"

    volumes:
      - ollama_data:/root/.ollama

    networks:
      - medical-app-network

    environment:
      - OLLAMA_HOST=0.0.0.0

    # Enable GPU support if available
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    # Uncomment if you want to use local Ollama instead of external
    # profiles:
    #   - local-ollama

  # Optional: Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    container_name: medical-chat-nginx
    restart: unless-stopped

    ports:
      - "80:80"
      - "443:443"

    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro

    networks:
      - medical-app-network

    depends_on:
      - medical-chat-app

    # Uncomment for production with reverse proxy
    profiles:
      - production

volumes:
  qdrant_data:
    driver: local
  ollama_data:
    driver: local

networks:
  medical-app-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16